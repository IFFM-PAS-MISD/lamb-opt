Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@phdthesis{Hariharan2015a,
abstract = {Object recognition in computer vision comes in many flavors, two of the most popular being object detection and semantic segmentation. Object detection systems detect every instance of a category in an image, and coarsely localize each with a bounding box. Semantic segmentation systems assign category labels to pixels, thus providing pixel-precise localization but failing to resolve individual instances of the category. We argue for a richer output: recognition systems should detect individual instances of a category and provide pixel precise segmentations for each, a task we call Simultaneous Detection and Segmentation or SDS. We describe approaches to this task that leverage convolutional neural networks for precise localization. We also show that the techniques we develop are also effective for other tasks such as segmenting the parts of a detected object or localizing its keypoints. These are our first steps towards a recognition system that goes beyond category labels and coarse bounding boxes to precise, detailed descriptions of objects in images.},
author = {Hariharan, Bharath},
file = {::},
pages = {39},
school = {University of California, Berkeley},
title = {{Beyond Bounding Boxes : Precise Localization of Objects in Images}},
url = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf},
year = {2015}
}
@article{Zhao2012a,
author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
file = {::},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
title = {{2012 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2012}},
year = {2012}
}
@techreport{Arbelaeza,
abstract = {We address the problem of segmenting and recognizing objects in real world images, focusing on challenging articulated categories such as humans and other animals. For this purpose, we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues. Our detectors produce class-specific scores for bottom-up regions, and then aggregate the votes of multiple overlapping candidates through pixel classification. We evaluate our approach on the PASCAL segmentation challenge, and report competitive performance with respect to current leading techniques. On VOC2010, our method obtains the best results in 6/20 categories and the highest performance on articulated objects.},
author = {Arbel{\'{a}}ez, Pablo and Hariharan, Bharath and Gu, Chunhui and Gupta, Saurabh and Bourdev, Lubomir and Malik, Jitendra},
file = {::},
title = {{Semantic Segmentation using Regions and Parts}}
}
@article{Fang2019a,
abstract = {Multi-atlas-based methods are commonly used for MR brain image labeling, which alleviates the burdening and time-consuming task of manual labeling in neuroimaging analysis studies. Traditionally, multi-atlas-based methods first register multiple atlases to the target image, and then propagate the labels from the labeled atlases to the unlabeled target image. However, the registration step involves non-rigid alignment, which is often time-consuming and might lack high accuracy. Alternatively, patch-based methods have shown promise in relaxing the demand for accurate registration, but they often require the use of hand-crafted features. Recently, deep learning techniques have demonstrated their effectiveness in image labeling, by automatically learning comprehensive appearance features from training images. In this paper, we propose a multi-atlas guided fully convolutional network (MA-FCN) for automatic image labeling, which aims at further improving the labeling performance with the aid of prior knowledge from the training atlases. Specifically, we train our MA-FCN model in a patch-based manner, where the input data consists of not only a training image patch but also a set of its neighboring (i.e., most similar) affine-aligned atlas patches. The guidance information from neighboring atlas patches can help boost the discriminative ability of the learned FCN. Experimental results on different datasets demonstrate the effectiveness of our proposed method, by significantly outperforming the conventional FCN and several state-of-the-art MR brain labeling methods.},
author = {Fang, Longwei and Zhang, Lichi and Nie, Dong and Cao, Xiaohuan and Rekik, Islem and Lee, Seong-Whan and He, Huiguang and Shen, Dinggang},
doi = {10.1016/j.media.2018.10.012},
file = {:E$\backslash$:/work/Mendeley/Fang et al. - 2019 - Automatic brain labeling via multi-atlas guided fully convolutional networks.pdf:pdf},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Brain image labeling,Fully convolutional network,Multi-atlas-based method,Patch-based labeling},
month = {jan},
pages = {157--168},
title = {{Automatic brain labeling via multi-atlas guided fully convolutional networks}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518308600},
volume = {51},
year = {2019}
}
@article{Rubio2019a,
abstract = {Structural Health Monitoring (SHM) has benefited from computer vision and more recently, Deep Learning approaches, to accurately estimate the state of deterioration of infrastructure. In our work, we test Fully Convolutional Networks (FCNs) with a dataset of deck areas of bridges for damage segmentation. We create a dataset for delamination and rebar exposure that has been collected from inspection records of bridges in Niigata Prefecture, Japan. The dataset consists of 734 images with three labels per image, which makes it the largest dataset of images of bridge deck damage. This data allows us to estimate the performance of our method based on regions of agreement, which emulates the uncertainty of in-field inspections. We demonstrate the practicality of FCNs to perform automated semantic segmentation of surface damages. Our model achieves a mean accuracy of 89.7{\%} for delamination and 78.4{\%} for rebar exposure, and a weighted F1 score of 81.9{\%}.},
author = {Rubio, Juan Jose and Kashiwa, Takahiro and Laiteerapong, Teera and Deng, Wenlong and Nagai, Kohei and Escalera, Sergio and Nakayama, Kotaro and Matsuo, Yutaka and Prendinger, Helmut},
doi = {10.1016/j.compind.2019.08.002},
file = {:E$\backslash$:/work/Mendeley/Rubio et al. - 2019 - Multi-class structural damage segmentation using fully convolutional networks.pdf:pdf},
issn = {01663615},
journal = {Computers in Industry},
keywords = {Bridge damage detection,Deep learning,Semantic segmentation},
month = {nov},
pages = {103121},
title = {{Multi-class structural damage segmentation using fully convolutional networks}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0166361518308911},
volume = {112},
year = {2019}
}
@article{Badrinarayanan2017a,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1511.00561},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/TPAMI.2016.2644615},
eprint = {1511.00561},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Deep convolutional neural networks,decoder,encoder,indoor scenes,pooling,road scenes,semantic pixel-wise segmentation,upsampling},
month = {dec},
number = {12},
pages = {2481--2495},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {https://ieeexplore.ieee.org/document/7803544/},
volume = {39},
year = {2017}
}
@article{Zhou2018a,
abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
archivePrefix = {arXiv},
arxivId = {1807.10165},
author = {Zhou, Zongwei and {Rahman Siddiquee}, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
doi = {10.1007/978-3-030-00889-5_1},
eprint = {1807.10165},
file = {::},
isbn = {9783030008888},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {3--11},
title = {{Unet++: A nested u-net architecture for medical image segmentation}},
volume = {11045 LNCS},
year = {2018}
}
@inproceedings{Zaitoun2015a,
abstract = {Due to the advent of computer technology image-processing techniques have become increasingly important in a wide variety of applications. Image segmentation is a classic subject in the field of image processing and also is a hotspot and focus of image processing techniques. Several general-purpose algorithms and techniques have been developed for image segmentation. Since there is no general solution to the image segmentation problem, these techniques often have to be combined with domain knowledge in order to effectively solve an image segmentation problem for a problem domain. This paper presents a comparative study of the basic Block-Based image segmentation techniques.},
author = {Zaitoun, Nida M. and Aqel, Musbah J.},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2015.09.027},
file = {::},
issn = {18770509},
keywords = {Image analysis,Image engineering,Image processing,Image segmentation,Image understanding},
pages = {797--806},
publisher = {Elsevier},
title = {{Survey on Image Segmentation Techniques}},
volume = {65},
year = {2015}
}
@article{Shelhamer2017a,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30{\%} relative improvement to 67.2{\%} mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
month = {apr},
number = {4},
pages = {640--651},
publisher = {IEEE Computer Society},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
volume = {39},
year = {2017}
}
@article{Ronneberger2015a,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
eprint = {1505.04597},
file = {::},
month = {may},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
year = {2015}
}
@article{Daia,
abstract = {The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs) [13]. The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a single image, which is time-consuming. In this paper, we propose to exploit shape information via masking convolutional features. The proposal segments (e.g., super-pixels) are treated as masks on the convolu-tional feature maps. The CNN features of segments are directly masked out from these maps and used to train clas-sifiers for recognition. We further propose a joint method to handle objects and "stuff" (e.g., grass, sky, water) in the same framework. State-of-the-art results are demonstrated on benchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling computational speed.},
author = {Dai, Jifeng and He, Kaiming and Sun, Jian},
file = {::},
title = {{Convolutional Feature Masking for Joint Object and Stuff Segmentation}},
url = {https://github.com/ShaoqingRen/SPP{\_}net}
}
